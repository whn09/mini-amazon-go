{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#how to setup rtmp server, please reference below url\n",
    "https://www.cnblogs.com/nowgood/p/ffmpegnginx.html\n",
    "\n",
    "#send rtmp data from pi4 to server, can use follow command\n",
    "ffmpeg -f v4l2 -r 24 -video_size vga -pix_fmt yuv420p12be -i /dev/video0 -b:v 500k -c:v h264_omx -preset ultrafast -an -f flv rtmp://52.80.130.52/live/123456\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gluoncv -i https://opentuna.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "import numpy as np\n",
    "import cv2\n",
    "import base64\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "import time\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# sagemaker_endpoint_name = 'object-detection-2019-10-18-17-56-41-925'\n",
    "# sagemaker_endpoint = sagemaker.predictor.RealTimePredictor(sagemaker_endpoint_name)\n",
    "# sagemaker_endpoint.content_type = 'image/jpeg'\n",
    "\n",
    "from object_detection import ObjectDetection\n",
    "objectDetection = ObjectDetection()\n",
    "\n",
    "for i in range(10):\n",
    "    # use GluonCV\n",
    "    detect_start = time.time()\n",
    "    frame = cv2.imread('../images/shenzhen_v1/frame_1604547852.3077297.jpg')\n",
    "#     class_IDs, scores, bounding_boxes = objectDetection.detect_image(frame)\n",
    "    class_IDs, scores, bounding_boxes = objectDetection.detect_image_yolo(frame)\n",
    "    #print('class_IDs:', class_IDs)\n",
    "    #print('scores:', scores)\n",
    "    #print('bounding_boxes:', bounding_boxes)\n",
    "    detect_end = time.time()\n",
    "    print('detect time:', detect_end-detect_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPlayer(object):\n",
    "    def __init__(self):\n",
    "        self._init = False\n",
    "        self._myImage = None\n",
    "        \n",
    "    def __call__(self, frame):\n",
    "        if frame is None:\n",
    "            return\n",
    "        if self._init is False:\n",
    "            self.init_display(frame)\n",
    "            self._init = True\n",
    "        else:\n",
    "            self.update_display(frame)\n",
    "\n",
    "    def init_display(self, frame):\n",
    "        assert frame is not None\n",
    "        frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA) # because Bokeh expects a RGBA image\n",
    "#         frame=cv2.flip(frame, -1) # because Bokeh flips vertically\n",
    "        frame=cv2.flip(frame, 0) # because Bokeh flips vertically\n",
    "        width=frame.shape[1]\n",
    "        height=frame.shape[0]\n",
    "        p = figure(x_range=(0,width), y_range=(0,height), output_backend=\"webgl\", width=width, height=height)\n",
    "        frame = np.array(frame)  # For bokeh>=2.2.0\n",
    "        frame = frame.view(\"uint32\").reshape(frame.shape[:2])  # For bokeh>=2.2.0\n",
    "        self._myImage = p.image_rgba(image=[frame], x=0, y=0, dw=width, dh=height)\n",
    "        show(p, notebook_handle=True)\n",
    "    \n",
    "    def update_display(self, frame):\n",
    "        assert frame is not None\n",
    "        frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "#         frame=cv2.flip(frame, -1)\n",
    "        frame=cv2.flip(frame, 0) \n",
    "        frame = np.array(frame)  # For bokeh>=2.2.0\n",
    "        frame = frame.view(\"uint32\").reshape(frame.shape[:2])  # For bokeh>=2.2.0\n",
    "        self._myImage.data_source.data['image']=[frame]\n",
    "        push_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_result_process(frame, classes, class_IDs, scores, bounding_boxes, hand_cnt, no_hand_cnt, start_trans, in_trans, curr_item_cnt, max_item_cnt, pre_msg, pre_msg2):\n",
    "    thres = 0.45\n",
    "    max_thres = 0.7\n",
    "    if class_IDs is not None and len(class_IDs) == 1:\n",
    "            if len(class_IDs[0]) >= 1:\n",
    "                hand = False\n",
    "                item_cnt = [0, 0]\n",
    "                for i in range(len(class_IDs[0])):\n",
    "                    if scores[0][i] > thres:\n",
    "                        class_ID = int(class_IDs[0][i])\n",
    "                        score = float(scores[0][i])\n",
    "                        bounding_box = bounding_boxes[0][i]\n",
    "                        # print('class_ID:', class_ID, 'score:', score, 'bounding_box:', bounding_box)\n",
    "                        xmin, ymin, xmax, ymax = [int(x) for x in bounding_box]\n",
    "                        if class_ID == 2 or scores[0][i] > max_thres:\n",
    "                            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color=(0, 255, 0), thickness=1)\n",
    "                            cv2.putText(frame, str(classes[class_ID])+':'+str(round(score, 2)), (xmin+10, ymin+10), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, thickness=1, color=(255, 255, 255))\n",
    "                        \n",
    "                        if class_ID == 2:\n",
    "                            hand = True\n",
    "                        else:\n",
    "                            if scores[0][i] > max_thres:\n",
    "                                item_cnt[class_ID] += 1\n",
    "                        \n",
    "                for i in range(len(item_cnt)):\n",
    "                    item_cnt[i] = min(item_cnt[i], max_item_cnt[i])\n",
    "                        \n",
    "                if not hand:\n",
    "                    no_hand_cnt += 1\n",
    "                else:\n",
    "                    hand_cnt += 1\n",
    "                    \n",
    "                if hand_cnt >= 3:\n",
    "                    start_trans = True\n",
    "                    in_trans = True\n",
    "                    hand_cnt = 0\n",
    "                    no_hand_cnt = 0\n",
    "                elif no_hand_cnt >= 3:\n",
    "                    start_trans = False\n",
    "                    hand_cnt = 0\n",
    "                    no_hand_cnt = 0\n",
    "                    \n",
    "                if start_trans:\n",
    "                    msg = 'Start Transaction'\n",
    "                else:\n",
    "                    msg = 'End Transaction'\n",
    "                    if in_trans:\n",
    "                        for i in range(len(item_cnt)):\n",
    "                            change_item = item_cnt[i]-curr_item_cnt[i]\n",
    "                            if change_item != 0:\n",
    "                                char = ''\n",
    "                                if change_item > 0:\n",
    "                                    char = '+'\n",
    "                                msg += ' '+classes[i]+': '+char+str(change_item)\n",
    "                        in_trans = False\n",
    "                        curr_item_cnt = item_cnt\n",
    "                    else:\n",
    "                        msg = pre_msg\n",
    "                    \n",
    "                    if not hand:\n",
    "                        for i in range(len(item_cnt)):\n",
    "                            curr_item_cnt[i] = max(curr_item_cnt[i], item_cnt[i])\n",
    "\n",
    "                msg2 = ''\n",
    "                for i in range(len(curr_item_cnt)):\n",
    "                    msg2 += classes[i]+': '+str(curr_item_cnt[i])+' '\n",
    "                \n",
    "                cv2.putText(frame, msg, (40, 40), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, thickness=2, color=(255, 255, 255))\n",
    "                cv2.putText(frame, msg2, (40, 60), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, thickness=1, color=(255, 255, 255))\n",
    "    return frame, hand_cnt, no_hand_cnt, start_trans, in_trans, curr_item_cnt, msg, msg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "need add rtmp support for sagemaker, please follow this page\n",
    "https://answers.opencv.org/question/180776/build-opencv-with-ffmpeg-support/\n",
    "\"\"\"\n",
    "# url=\"rtmp://52.80.130.52/live/123456\"\n",
    "#url=\"rtmp://52.81.133.221:1935/live/123456\"\n",
    "url=\"rtmp://140.179.5.148:1935/live/123456\"\n",
    "\n",
    "vcap = cv2.VideoCapture(url)\n",
    "player = VideoPlayer()\n",
    "\n",
    "hand_cnt = 0\n",
    "no_hand_cnt = 0\n",
    "start_trans = False\n",
    "in_trans = False\n",
    "curr_item_cnt = [0, 0]\n",
    "max_item_cnt = [2, 2]\n",
    "pre_msg = ''\n",
    "pre_msg2 = ''\n",
    "run_mode = 2  # 1 for original image, 2 for object detection, 3 for original video\n",
    "fps = 8  # 8, 12, 24\n",
    "\n",
    "print(\"Video start\")\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    read_start = time.time()\n",
    "    if run_mode == 1:\n",
    "        for i in range(fps):\n",
    "            ret, frame = vcap.read()\n",
    "    elif run_mode == 2:\n",
    "        ret, frame = vcap.read()\n",
    "#         ret, frame = vcap.read()\n",
    "#         ret, frame = vcap.read()\n",
    "    elif run_mode == 3:\n",
    "        ret, frame = vcap.read()\n",
    "    read_end = time.time()\n",
    "#     print('read time:', read_end-read_start)\n",
    "\n",
    "    if frame is not None:\n",
    "        start = time.time()\n",
    "        frame = cv2.flip(frame, -1)\n",
    "        \n",
    "        # save image\n",
    "        if run_mode == 1:\n",
    "            filename = '../images/test/frame_'+str(time.time())+'.jpg'\n",
    "            cv2.imwrite(filename, frame)\n",
    "            print(filename)\n",
    "            t = 1\n",
    "            time.sleep(t)\n",
    "        elif run_mode == 2:\n",
    "            # use SageMaker\n",
    "            #with open(filename, 'rb') as image:\n",
    "            #    f = image.read()\n",
    "            #    b = bytearray(f)\n",
    "            #result = sagemaker_endpoint.predict(b)\n",
    "            #print(json.loads(result))\n",
    "\n",
    "            # use GluonCV\n",
    "            detect_start = time.time()\n",
    "#             class_IDs, scores, bounding_boxes = objectDetection.detect_image(frame)\n",
    "            class_IDs, scores, bounding_boxes = objectDetection.detect_image_yolo(frame)\n",
    "            #print('class_IDs:', class_IDs)\n",
    "            #print('scores:', scores)\n",
    "            #print('bounding_boxes:', bounding_boxes)\n",
    "            detect_end = time.time()\n",
    "#             print('detect time:', detect_end-detect_start)\n",
    "\n",
    "            frame, hand_cnt, no_hand_cnt, start_trans, in_trans, curr_item_cnt, msg, msg2 = detection_result_process(frame, objectDetection.classes, class_IDs, scores, bounding_boxes, hand_cnt, no_hand_cnt, start_trans, in_trans, curr_item_cnt, max_item_cnt, pre_msg, pre_msg2)\n",
    "#             print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(detect_end)), 'msg:', msg)\n",
    "#             print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(detect_end)), 'msg2:', msg2)\n",
    "            pre_msg = msg\n",
    "            pre_msg2 = msg2\n",
    "\n",
    "            # Display the resulting frame\n",
    "#             filename = '../images/test/frame_'+str(time.time())+'.jpg'\n",
    "#             cv2.imwrite(filename, frame)\n",
    "            player(frame)\n",
    "            end = time.time()\n",
    "#             print('all time:', end-start)\n",
    "\n",
    "            #t = 1\n",
    "            #time.sleep(t)\n",
    "            #vcap.set(cv2.CAP_PROP_POS_FRAMES, t*25)\n",
    "\n",
    "            #t = 0.2\n",
    "            #sleep_time = t-(end-start)\n",
    "            #if sleep_time >= 0:\n",
    "            #    time.sleep(sleep_time)\n",
    "            #    vcap.set(cv2.CAP_PROP_POS_FRAMES, sleep_time*25)\n",
    "\n",
    "            #skip_frame = max(0, 25-int(1/(end-start)))\n",
    "            #print('skip_frame:', skip_frame)\n",
    "            #if skip_frame > 0:\n",
    "            #    vcap.set(cv2.CAP_PROP_POS_FRAMES, skip_frame)\n",
    "        elif run_mode == 3:\n",
    "            player(frame)\n",
    "            \n",
    "        #vcap.set(cv2.CAP_PROP_POS_FRAMES, t*25)\n",
    "        #break\n",
    "\n",
    "        # Press q to close the video windows before it ends if you want\n",
    "        #if cv2.waitKey(22) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    else:\n",
    "        print(\"Frame is None\")\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "vcap.release()\n",
    "print(\"Video stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
